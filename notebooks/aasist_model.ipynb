{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyykPtg2jtvm",
        "outputId": "dd0d27f3-f509-422a-feca-14091de59d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "active_device = get_active_device()\n",
        "print(active_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPImYChMPMzz",
        "outputId": "545b2186-5c28-4704-f324-35ca790c0ff2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Parameters\n",
        "# A random sub sample of the LA data was used for this experiment.\n",
        "# The LA data was used following most of the academic literture,\n",
        "# which deals with this dataset.\n",
        "# The sub-sampling was done due to huge running times.\n",
        "# The dataset was split as follows:\n",
        "# Train Set - 80% (25k -> 20k)\n",
        "# Dev Set   - 40% (25k -> 10k)\n",
        "# Eval Set  - 14% (70k -> 10k)\n",
        "\n",
        "config = {\n",
        "    \"train_protocol\":\"drive/MyDrive/AntiSpoofing/sub_sample/train_protocol.txt\",\n",
        "    \"dev_protocol\":\"drive/MyDrive/AntiSpoofing/sub_sample/dev_protocol.txt\",\n",
        "    \"eval_protocol\":\"drive/MyDrive/AntiSpoofing/sub_sample/eval_protocol.txt\",\n",
        "    \"train_audio_folder\":\"drive/MyDrive/AntiSpoofing/sub_sample/train/\",\n",
        "    \"dev_audio_folder\":\"drive/MyDrive/AntiSpoofing/sub_sample/dev/\",\n",
        "    \"eval_audio_folder\":\"drive/MyDrive/AntiSpoofing/sub_sample/eval/\",\n",
        "    \"max_speech_length\":64600,\n",
        "    \"batch_size\": 24,\n",
        "    \"num_epochs\": 15,\n",
        "    \"min_valid_epochs\":3,\n",
        "    \"early_stop_max_no_imp\":3,\n",
        "    \"cudnn_deterministic_toggle\": \"True\",\n",
        "    \"cudnn_benchmark_toggle\": \"False\",\n",
        "    \"model_config\": {\n",
        "        \"architecture\": \"AASIST\",\n",
        "        \"nb_samp\": 48000,\n",
        "        \"first_conv\": 128,\n",
        "        \"filters\": [70, [1, 32], [32, 32], [32, 24], [24, 24]],\n",
        "        \"gat_dims\": [24, 32],\n",
        "        \"pool_ratios\": [0.4, 0.5, 0.7, 0.5],\n",
        "        \"temperatures\": [2.0, 2.0, 100.0, 100.0]\n",
        "    },\n",
        "    \"optim_config\": {\n",
        "        \"optimizer\": \"adam\", \n",
        "        \"amsgrad\": \"False\",\n",
        "        \"base_lr\": 0.0001,\n",
        "        \"lr_min\": 0.000005,\n",
        "        \"betas\": [0.9, 0.999],\n",
        "        \"weight_decay\": 0.0001,\n",
        "        \"scheduler\": \"cosine\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "K27g6dKEkQic"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils functions\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "\n",
        "## Adopted from https://github.com/clovaai/aasist\n",
        "\n",
        "# The model and its associated configuration and utility\n",
        "# were taken from the aticle, which came alone its code:\n",
        "#\n",
        "# AASIST: AUDIO ANTI-SPOOFING USING INTEGRATED\n",
        "#         SPECTRO-TEMPORAL GRAPH ATTENTION NETWORKS\n",
        "# by: Jee-weon Jung et co, 2021\n",
        "\n",
        "\n",
        "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
        "    \"\"\"Cosine Annealing for learning rate decay scheduler\"\"\"\n",
        "    return lr_min + (lr_max -\n",
        "                     lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    \"\"\"\n",
        "    Used in generating seed for the worker of torch.utils.data.Dataloader\n",
        "    \"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "class SGDRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"SGD with restarts scheduler\"\"\"\n",
        "    def __init__(self, optimizer, T0, T_mul, eta_min, last_epoch=-1):\n",
        "        self.Ti = T0\n",
        "        self.T_mul = T_mul\n",
        "        self.eta_min = eta_min\n",
        "\n",
        "        self.last_restart = 0\n",
        "\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        T_cur = self.last_epoch - self.last_restart\n",
        "        if T_cur >= self.Ti:\n",
        "            self.last_restart = self.last_epoch\n",
        "            self.Ti = self.Ti * self.T_mul\n",
        "            T_cur = 0\n",
        "\n",
        "        return [\n",
        "            self.eta_min + (base_lr - self.eta_min) *\n",
        "            (1 + np.cos(np.pi * T_cur / self.Ti)) / 2\n",
        "            for base_lr in self.base_lrs\n",
        "        ]\n",
        "\n",
        "\n",
        "def _get_optimizer(model_parameters, optim_config):\n",
        "    \"\"\"Defines optimizer according to the given config\"\"\"\n",
        "    optimizer_name = optim_config['optimizer']\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model_parameters,\n",
        "                                    lr=optim_config['base_lr'],\n",
        "                                    momentum=optim_config['momentum'],\n",
        "                                    weight_decay=optim_config['weight_decay'],\n",
        "                                    nesterov=optim_config['nesterov'])\n",
        "    elif optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model_parameters,\n",
        "                                     lr=optim_config['base_lr'],\n",
        "                                     betas=optim_config['betas'],\n",
        "                                     weight_decay=optim_config['weight_decay'],\n",
        "                                     amsgrad=optim_config['amsgrad'])\n",
        "    else:\n",
        "        print('Un-known optimizer', optimizer_name)\n",
        "        sys.exit()\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def _get_scheduler(optimizer, optim_config):\n",
        "    \"\"\"\n",
        "    Defines learning rate scheduler according to the given config\n",
        "    \"\"\"\n",
        "    if optim_config['scheduler'] == 'multistep':\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=optim_config['milestones'],\n",
        "            gamma=optim_config['lr_decay'])\n",
        "\n",
        "    elif optim_config['scheduler'] == 'sgdr':\n",
        "        scheduler = SGDRScheduler(optimizer, optim_config['T0'],\n",
        "                                  optim_config['Tmult'],\n",
        "                                  optim_config['lr_min'])\n",
        "\n",
        "    elif optim_config['scheduler'] == 'cosine':\n",
        "        total_steps = optim_config['epochs'] * optim_config['steps_per_epoch']\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer,\n",
        "            lr_lambda=lambda step: cosine_annealing(\n",
        "                step,\n",
        "                total_steps,\n",
        "                1,  # since lr_lambda computes multiplicative factor\n",
        "                optim_config['lr_min'] / optim_config['base_lr']))\n",
        "\n",
        "    else:\n",
        "        scheduler = None\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def create_optimizer(model_parameters, optim_config):\n",
        "    \"\"\"Defines an optimizer and a scheduler\"\"\"\n",
        "    optimizer = _get_optimizer(model_parameters, optim_config)\n",
        "    scheduler = _get_scheduler(optimizer, optim_config)\n",
        "    return optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "AJ6IaR1FkE-p"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4XuEPN9RNZhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading utilities\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "## Adapted from \"Hemlata Tak, Jee-weon Jung - tak@eurecom.fr, jeeweon.jung@navercorp.com\"\n",
        "\n",
        "AUDIO_FILE_FIELD = 1\n",
        "ATTACK_TYPE_FIELD = 3\n",
        "LABEL_FIELD = 4\n",
        "LABELS_MAP = {\"bonafide\":1, \"spoof\":0}\n",
        "MAX_SPEECH_LENGTH = 64600\n",
        "\n",
        "def pad(x, max_len=64600):\n",
        "    x_len = x.shape[0]\n",
        "    if x_len >= max_len:\n",
        "        return x[:max_len]\n",
        "    # need to pad\n",
        "    num_repeats = int(max_len / x_len) + 1\n",
        "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
        "    return padded_x\n",
        "\n",
        "\n",
        "def pad_random(x: np.ndarray, max_len: int = 64600):\n",
        "    x_len = x.shape[0]\n",
        "    # if duration is already long enough\n",
        "    if x_len >= max_len:\n",
        "        stt = np.random.randint(x_len - max_len)\n",
        "        return x[stt:stt + max_len]\n",
        "\n",
        "    # if too short\n",
        "    num_repeats = int(max_len / x_len) + 1\n",
        "    padded_x = np.tile(x, (num_repeats))[:max_len]\n",
        "    return padded_x\n",
        "\n",
        "class Dataset_ASVspoof2019(Dataset):\n",
        "    def __init__(self, config: dict, sample_name: str):\n",
        "        # Read the data set protocol file.\n",
        "        protocol_file = config[sample_name + \"_protocol\"]\n",
        "        audio_files_folder = config[sample_name + \"_audio_folder\"]\n",
        "        with open(protocol_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            data_set_items = [x.strip().split() for x in f.readlines()]\n",
        "\n",
        "        self.max_speech_length = config['max_speech_length']\n",
        "        self.audio_files = [x[AUDIO_FILE_FIELD] for x in data_set_items]\n",
        "        self.labels = [LABELS_MAP[x[LABEL_FIELD]] for x in data_set_items]\n",
        "        self.audio_files_folder = audio_files_folder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_file = self.audio_files[index]\n",
        "        signal, _ = sf.read(self.audio_files_folder + audio_file + \".flac\")\n",
        "        padded_signal = pad_random(signal, self.max_speech_length)\n",
        "        tensor_signal = Tensor(padded_signal)\n",
        "        y = self.labels[index]\n",
        "        return tensor_signal, y\n"
      ],
      "metadata": {
        "id": "6HQvNGcLkdIP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data example.\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "ds = Dataset_ASVspoof2019(config, \"train\")\n",
        "dl = DataLoader(ds, batch_size=config['batch_size'], shuffle=True, drop_last=False, pin_memory=False)\n",
        "\n",
        "print(\"DataSet:\")\n",
        "print(\"#Items: \" + str(len(ds)))\n",
        "for signal, label in ds:\n",
        "    break\n",
        "print(\"Signal:\")\n",
        "print(signal.shape)\n",
        "print(signal[:10])\n",
        "print(\"Label: \" + str(label))\n",
        "\n",
        "print(\"\\nDataLoader:\")\n",
        "print(\"#Items: \" + str(len(dl)))\n",
        "for batch, label in dl:\n",
        "    break\n",
        "print(\"batch: \" + str(batch.shape))\n",
        "print(\"labels: \" + str(label[:10]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWasVztqLSJV",
        "outputId": "67c94844-ebac-4a69-97d8-e1ebeafc0d3c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataSet:\n",
            "#Items: 20288\n",
            "Signal:\n",
            "torch.Size([64600])\n",
            "tensor([0.0018, 0.0018, 0.0017, 0.0017, 0.0016, 0.0015, 0.0015, 0.0015, 0.0016,\n",
            "        0.0015])\n",
            "Label: 1\n",
            "\n",
            "DataLoader:\n",
            "#Items: 846\n",
            "batch: torch.Size([24, 64600])\n",
            "labels: tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The classifier model.\n",
        "# The code was taken from the implementation\n",
        "# of the above mentioned paper.\n",
        "# The main principles of this model:\n",
        "# (1) End-to-End - fed by the raw audio signal\n",
        "# (2) Model both the spectogram and the temporal characteristics of the signal\n",
        "# (3) A quite deep network with graph attention layer\n",
        "\n",
        "import random\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "## Adopted from https://github.com/clovaai/aasist\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # attention map\n",
        "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
        "        self.att_weight = self._init_new_params(out_dim, 1)\n",
        "\n",
        "        # project\n",
        "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # batch norm\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "        # dropout for inputs\n",
        "        self.input_drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # activate\n",
        "        self.act = nn.SELU(inplace=True)\n",
        "\n",
        "        # temperature\n",
        "        self.temp = 1.\n",
        "        if \"temperature\" in kwargs:\n",
        "            self.temp = kwargs[\"temperature\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x   :(#bs, #node, #dim)\n",
        "        '''\n",
        "        # apply input dropout\n",
        "        x = self.input_drop(x)\n",
        "\n",
        "        # derive attention map\n",
        "        att_map = self._derive_att_map(x)\n",
        "\n",
        "        # projection\n",
        "        x = self._project(x, att_map)\n",
        "\n",
        "        # apply batch norm\n",
        "        x = self._apply_BN(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "    def _pairwise_mul_nodes(self, x):\n",
        "        '''\n",
        "        Calculates pairwise multiplication of nodes.\n",
        "        - for attention map\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, #dim)\n",
        "        '''\n",
        "\n",
        "        nb_nodes = x.size(1)\n",
        "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
        "        x_mirror = x.transpose(1, 2)\n",
        "\n",
        "        return x * x_mirror\n",
        "\n",
        "    def _derive_att_map(self, x):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = self._pairwise_mul_nodes(x)\n",
        "        # size: (#bs, #node, #node, #dim_out)\n",
        "        att_map = torch.tanh(self.att_proj(att_map))\n",
        "        # size: (#bs, #node, #node, 1)\n",
        "        att_map = torch.matmul(att_map, self.att_weight)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _project(self, x, att_map):\n",
        "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
        "        x2 = self.proj_without_att(x)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _apply_BN(self, x):\n",
        "        org_size = x.size()\n",
        "        x = x.view(-1, org_size[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(org_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_new_params(self, *size):\n",
        "        out = nn.Parameter(torch.FloatTensor(*size))\n",
        "        nn.init.xavier_normal_(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HtrgGraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
        "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
        "\n",
        "        # attention map\n",
        "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
        "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
        "\n",
        "        # project\n",
        "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # batch norm\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "        # dropout for inputs\n",
        "        self.input_drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # activate\n",
        "        self.act = nn.SELU(inplace=True)\n",
        "\n",
        "        # temperature\n",
        "        self.temp = 1.\n",
        "        if \"temperature\" in kwargs:\n",
        "            self.temp = kwargs[\"temperature\"]\n",
        "\n",
        "    def forward(self, x1, x2, master=None):\n",
        "        '''\n",
        "        x1  :(#bs, #node, #dim)\n",
        "        x2  :(#bs, #node, #dim)\n",
        "        '''\n",
        "        num_type1 = x1.size(1)\n",
        "        num_type2 = x2.size(1)\n",
        "\n",
        "        x1 = self.proj_type1(x1)\n",
        "        x2 = self.proj_type2(x2)\n",
        "\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "\n",
        "        if master is None:\n",
        "            master = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        # apply input dropout\n",
        "        x = self.input_drop(x)\n",
        "\n",
        "        # derive attention map\n",
        "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
        "\n",
        "        # directional edge for master node\n",
        "        master = self._update_master(x, master)\n",
        "\n",
        "        # projection\n",
        "        x = self._project(x, att_map)\n",
        "\n",
        "        # apply batch norm\n",
        "        x = self._apply_BN(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x1 = x.narrow(1, 0, num_type1)\n",
        "        x2 = x.narrow(1, num_type1, num_type2)\n",
        "\n",
        "        return x1, x2, master\n",
        "\n",
        "    def _update_master(self, x, master):\n",
        "\n",
        "        att_map = self._derive_att_map_master(x, master)\n",
        "        master = self._project_master(x, master, att_map)\n",
        "\n",
        "        return master\n",
        "\n",
        "    def _pairwise_mul_nodes(self, x):\n",
        "        '''\n",
        "        Calculates pairwise multiplication of nodes.\n",
        "        - for attention map\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, #dim)\n",
        "        '''\n",
        "\n",
        "        nb_nodes = x.size(1)\n",
        "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
        "        x_mirror = x.transpose(1, 2)\n",
        "\n",
        "        return x * x_mirror\n",
        "\n",
        "    def _derive_att_map_master(self, x, master):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = x * master\n",
        "        att_map = torch.tanh(self.att_projM(att_map))\n",
        "\n",
        "        att_map = torch.matmul(att_map, self.att_weightM)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _derive_att_map(self, x, num_type1, num_type2):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = self._pairwise_mul_nodes(x)\n",
        "        # size: (#bs, #node, #node, #dim_out)\n",
        "        att_map = torch.tanh(self.att_proj(att_map))\n",
        "        # size: (#bs, #node, #node, 1)\n",
        "\n",
        "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
        "\n",
        "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
        "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
        "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
        "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
        "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
        "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
        "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
        "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
        "\n",
        "        att_map = att_board\n",
        "\n",
        "        # att_map = torch.matmul(att_map, self.att_weight12)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _project(self, x, att_map):\n",
        "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
        "        x2 = self.proj_without_att(x)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _project_master(self, x, master, att_map):\n",
        "\n",
        "        x1 = self.proj_with_attM(torch.matmul(\n",
        "            att_map.squeeze(-1).unsqueeze(1), x))\n",
        "        x2 = self.proj_without_attM(master)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _apply_BN(self, x):\n",
        "        org_size = x.size()\n",
        "        x = x.view(-1, org_size[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(org_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_new_params(self, *size):\n",
        "        out = nn.Parameter(torch.FloatTensor(*size))\n",
        "        nn.init.xavier_normal_(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GraphPool(nn.Module):\n",
        "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.proj = nn.Linear(in_dim, 1)\n",
        "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
        "        self.in_dim = in_dim\n",
        "\n",
        "    def forward(self, h):\n",
        "        Z = self.drop(h)\n",
        "        weights = self.proj(Z)\n",
        "        scores = self.sigmoid(weights)\n",
        "        new_h = self.top_k_graph(scores, h, self.k)\n",
        "\n",
        "        return new_h\n",
        "\n",
        "    def top_k_graph(self, scores, h, k):\n",
        "        \"\"\"\n",
        "        args\n",
        "        =====\n",
        "        scores: attention-based weights (#bs, #node, 1)\n",
        "        h: graph data (#bs, #node, #dim)\n",
        "        k: ratio of remaining nodes, (float)\n",
        "\n",
        "        returns\n",
        "        =====\n",
        "        h: graph pool applied data (#bs, #node', #dim)\n",
        "        \"\"\"\n",
        "        _, n_nodes, n_feat = h.size()\n",
        "        n_nodes = max(int(n_nodes * k), 1)\n",
        "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
        "        idx = idx.expand(-1, -1, n_feat)\n",
        "\n",
        "        h = h * scores\n",
        "        h = torch.gather(h, 1, idx)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class CONV(nn.Module):\n",
        "    @staticmethod\n",
        "    def to_mel(hz):\n",
        "        return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_hz(mel):\n",
        "        return 700 * (10**(mel / 2595) - 1)\n",
        "\n",
        "    def __init__(self,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 sample_rate=16000,\n",
        "                 in_channels=1,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 bias=False,\n",
        "                 groups=1,\n",
        "                 mask=False):\n",
        "        super().__init__()\n",
        "        if in_channels != 1:\n",
        "\n",
        "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (\n",
        "                in_channels)\n",
        "            raise ValueError(msg)\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
        "        if kernel_size % 2 == 0:\n",
        "            self.kernel_size = self.kernel_size + 1\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.mask = mask\n",
        "        if bias:\n",
        "            raise ValueError('SincConv does not support bias.')\n",
        "        if groups > 1:\n",
        "            raise ValueError('SincConv does not support groups.')\n",
        "\n",
        "        NFFT = 512\n",
        "        f = int(self.sample_rate / 2) * np.linspace(0, 1, int(NFFT / 2) + 1)\n",
        "        fmel = self.to_mel(f)\n",
        "        fmelmax = np.max(fmel)\n",
        "        fmelmin = np.min(fmel)\n",
        "        filbandwidthsmel = np.linspace(fmelmin, fmelmax, self.out_channels + 1)\n",
        "        filbandwidthsf = self.to_hz(filbandwidthsmel)\n",
        "\n",
        "        self.mel = filbandwidthsf\n",
        "        self.hsupp = torch.arange(-(self.kernel_size - 1) / 2,\n",
        "                                  (self.kernel_size - 1) / 2 + 1)\n",
        "        self.band_pass = torch.zeros(self.out_channels, self.kernel_size)\n",
        "        for i in range(len(self.mel) - 1):\n",
        "            fmin = self.mel[i]\n",
        "            fmax = self.mel[i + 1]\n",
        "            hHigh = (2*fmax/self.sample_rate) * \\\n",
        "                np.sinc(2*fmax*self.hsupp/self.sample_rate)\n",
        "            hLow = (2*fmin/self.sample_rate) * \\\n",
        "                np.sinc(2*fmin*self.hsupp/self.sample_rate)\n",
        "            hideal = hHigh - hLow\n",
        "\n",
        "            self.band_pass[i, :] = Tensor(np.hamming(\n",
        "                self.kernel_size)) * Tensor(hideal)\n",
        "\n",
        "    def forward(self, x, mask=False):\n",
        "        band_pass_filter = self.band_pass.clone().to(x.device)\n",
        "        if mask:\n",
        "            A = np.random.uniform(0, 20)\n",
        "            A = int(A)\n",
        "            A0 = random.randint(0, band_pass_filter.shape[0] - A)\n",
        "            band_pass_filter[A0:A0 + A, :] = 0\n",
        "        else:\n",
        "            band_pass_filter = band_pass_filter\n",
        "\n",
        "        self.filters = (band_pass_filter).view(self.out_channels, 1,\n",
        "                                               self.kernel_size)\n",
        "\n",
        "        return F.conv1d(x,\n",
        "                        self.filters,\n",
        "                        stride=self.stride,\n",
        "                        padding=self.padding,\n",
        "                        dilation=self.dilation,\n",
        "                        bias=None,\n",
        "                        groups=1)\n",
        "\n",
        "\n",
        "class Residual_block(nn.Module):\n",
        "    def __init__(self, nb_filts, first=False):\n",
        "        super().__init__()\n",
        "        self.first = first\n",
        "\n",
        "        if not self.first:\n",
        "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
        "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
        "                               out_channels=nb_filts[1],\n",
        "                               kernel_size=(2, 3),\n",
        "                               padding=(1, 1),\n",
        "                               stride=1)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
        "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
        "                               out_channels=nb_filts[1],\n",
        "                               kernel_size=(2, 3),\n",
        "                               padding=(0, 1),\n",
        "                               stride=1)\n",
        "\n",
        "        if nb_filts[0] != nb_filts[1]:\n",
        "            self.downsample = True\n",
        "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
        "                                             out_channels=nb_filts[1],\n",
        "                                             padding=(0, 1),\n",
        "                                             kernel_size=(1, 3),\n",
        "                                             stride=1)\n",
        "\n",
        "        else:\n",
        "            self.downsample = False\n",
        "        self.mp = nn.MaxPool2d((1, 3))  # self.mp = nn.MaxPool2d((1,4))\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if not self.first:\n",
        "            out = self.bn1(x)\n",
        "            out = self.selu(out)\n",
        "        else:\n",
        "            out = x\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        # print('out',out.shape)\n",
        "        out = self.bn2(out)\n",
        "        out = self.selu(out)\n",
        "        # print('out',out.shape)\n",
        "        out = self.conv2(out)\n",
        "        #print('conv2 out',out.shape)\n",
        "        if self.downsample:\n",
        "            identity = self.conv_downsample(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.mp(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config[\"model_config\"]\n",
        "        filts = self.config[\"filters\"]\n",
        "        gat_dims = self.config[\"gat_dims\"]\n",
        "        pool_ratios = self.config[\"pool_ratios\"]\n",
        "        temperatures = self.config[\"temperatures\"]\n",
        "\n",
        "        self.conv_time = CONV(out_channels=filts[0],\n",
        "                              kernel_size=self.config[\"first_conv\"],\n",
        "                              in_channels=1)\n",
        "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
        "\n",
        "        self.drop = nn.Dropout(0.5, inplace=True)\n",
        "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
        "\n",
        "        self.pos_S = nn.Parameter(torch.randn(1, 23, filts[-1][-1]))\n",
        "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
        "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
        "\n",
        "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
        "                                               gat_dims[0],\n",
        "                                               temperature=temperatures[0])\n",
        "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
        "                                               gat_dims[0],\n",
        "                                               temperature=temperatures[1])\n",
        "\n",
        "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
        "\n",
        "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
        "\n",
        "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
        "\n",
        "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
        "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
        "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "\n",
        "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "\n",
        "        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n",
        "\n",
        "    def forward(self, x, Freq_aug=False):\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv_time(x, mask=Freq_aug)\n",
        "        x = x.unsqueeze(dim=1)\n",
        "        x = F.max_pool2d(torch.abs(x), (3, 3))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.selu(x)\n",
        "\n",
        "        # get embeddings using encoder\n",
        "        # (#bs, #filt, #spec, #seq)\n",
        "        e = self.encoder(x)\n",
        "\n",
        "        # spectral GAT (GAT-S)\n",
        "        e_S, _ = torch.max(torch.abs(e), dim=3)  # max along time\n",
        "        e_S = e_S.transpose(1, 2) + self.pos_S\n",
        "\n",
        "        gat_S = self.GAT_layer_S(e_S)\n",
        "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
        "\n",
        "        # temporal GAT (GAT-T)\n",
        "        e_T, _ = torch.max(torch.abs(e), dim=2)  # max along freq\n",
        "        e_T = e_T.transpose(1, 2)\n",
        "\n",
        "        gat_T = self.GAT_layer_T(e_T)\n",
        "        out_T = self.pool_T(gat_T)\n",
        "\n",
        "        # learnable master node\n",
        "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
        "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
        "\n",
        "        # inference 1\n",
        "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
        "            out_T, out_S, master=self.master1)\n",
        "\n",
        "        out_S1 = self.pool_hS1(out_S1)\n",
        "        out_T1 = self.pool_hT1(out_T1)\n",
        "\n",
        "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
        "            out_T1, out_S1, master=master1)\n",
        "        out_T1 = out_T1 + out_T_aug\n",
        "        out_S1 = out_S1 + out_S_aug\n",
        "        master1 = master1 + master_aug\n",
        "\n",
        "        # inference 2\n",
        "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
        "            out_T, out_S, master=self.master2)\n",
        "        out_S2 = self.pool_hS2(out_S2)\n",
        "        out_T2 = self.pool_hT2(out_T2)\n",
        "\n",
        "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
        "            out_T2, out_S2, master=master2)\n",
        "        out_T2 = out_T2 + out_T_aug\n",
        "        out_S2 = out_S2 + out_S_aug\n",
        "        master2 = master2 + master_aug\n",
        "\n",
        "        out_T1 = self.drop_way(out_T1)\n",
        "        out_T2 = self.drop_way(out_T2)\n",
        "        out_S1 = self.drop_way(out_S1)\n",
        "        out_S2 = self.drop_way(out_S2)\n",
        "        master1 = self.drop_way(master1)\n",
        "        master2 = self.drop_way(master2)\n",
        "\n",
        "        out_T = torch.max(out_T1, out_T2)\n",
        "        out_S = torch.max(out_S1, out_S2)\n",
        "        master = torch.max(master1, master2)\n",
        "\n",
        "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
        "        T_avg = torch.mean(out_T, dim=1)\n",
        "\n",
        "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
        "        S_avg = torch.mean(out_S, dim=1)\n",
        "\n",
        "        last_hidden = torch.cat(\n",
        "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
        "\n",
        "        last_hidden = self.drop(last_hidden)\n",
        "        output = self.out_layer(last_hidden)\n",
        "\n",
        "        return last_hidden, output\n"
      ],
      "metadata": {
        "id": "l8OQ1lUCFFDx"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EER computation function.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from sklearn import metrics\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def compute_eer(model: torch.nn.Module, test_set: DataLoader) -> tuple:\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    targets = []\n",
        "    for X, y in test_set:\n",
        "        X = X.to(active_device)\n",
        "        y = y.to(active_device)\n",
        "        with torch.no_grad():\n",
        "            _, logits = model(X)\n",
        "        curr_scores = torch.softmax(logits, dim=1)\n",
        "        curr_scores = curr_scores.data.detach().cpu().numpy()\n",
        "        curr_scores = curr_scores[:,1] - curr_scores[:,0]\n",
        "        curr_scores = curr_scores.clip(min=0)\n",
        "\n",
        "        scores.append(curr_scores)\n",
        "        targets.append(y.detach().cpu().numpy())\n",
        "\n",
        "    scores = np.concatenate(scores, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(targets, scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.absolute(fpr - fnr))\n",
        "        \n",
        "    return np.mean((fpr[eer_index], fnr[eer_index]))*100, (time.time() - t0)\n"
      ],
      "metadata": {
        "id": "Yw5gThnxEpHv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training procedure.\n",
        "\n",
        "!pip install torchcontrib\n",
        "print('torchcontrib installed!')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "from torchcontrib.optim import SWA\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "DEFAULT_MAX_EER = 1000\n",
        "SEED = 42\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        self.active_device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "\n",
        "    def train(self, config: dict) -> tuple:\n",
        "        print('AASIST trainer - start')\n",
        "\n",
        "        pending_model = Model(config)\n",
        "        pending_model = pending_model.to(active_device)\n",
        "        optimal_model = None\n",
        "\n",
        "        print(\"Load samples\")\n",
        "        train_dataset = Dataset_ASVspoof2019(config, \"train\")\n",
        "        dev_dataset = Dataset_ASVspoof2019(config, \"dev\")\n",
        "        eval_dataset = Dataset_ASVspoof2019(config, \"eval\")\n",
        "\n",
        "        print(\"Train Set size = \" + str(len(train_dataset)))\n",
        "        print(\"Dev Set size = \" + str(len(dev_dataset)))\n",
        "        print(\"Eval Set size = \" + str(len(eval_dataset)))\n",
        "\n",
        "        gen = torch.Generator()\n",
        "        gen.manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                  batch_size=config['batch_size'],\n",
        "                                  shuffle=True,\n",
        "                                  drop_last=True,\n",
        "                                  pin_memory=True,\n",
        "                                  worker_init_fn=seed_worker,\n",
        "                                  generator=gen)\n",
        "        \n",
        "        dev_loader = DataLoader(dev_dataset,\n",
        "                                batch_size=config['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                drop_last=False,\n",
        "                                pin_memory=True)\n",
        "        \n",
        "        eval_loader = DataLoader(eval_dataset,\n",
        "                                 batch_size=config['batch_size'],\n",
        "                                 shuffle=False,\n",
        "                                 drop_last=False,\n",
        "                                 pin_memory=True)\n",
        "\n",
        "        \n",
        "        print('set optimizer & loss')\n",
        "        optim_config = config[\"optim_config\"]\n",
        "        optim_config[\"epochs\"] = config[\"num_epochs\"]\n",
        "        optim_config[\"steps_per_epoch\"] = len(train_loader)\n",
        "        optimizer, scheduler = create_optimizer(pending_model.parameters(), optim_config)\n",
        "        optimizer_swa = SWA(optimizer)\n",
        "\n",
        "        weight = torch.FloatTensor([0.1, 0.9]).to(self.active_device)\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
        "        criterion = criterion.to(active_device)\n",
        "\n",
        "        best_dev_eer = DEFAULT_MAX_EER\n",
        "        best_dev_epoch = -1\n",
        "        best_eval_eer = DEFAULT_MAX_EER\n",
        "        best_eval_epoch = -1\n",
        "        \n",
        "        print('start training loops. #epochs = ' + str(config['num_epochs']))\n",
        "        print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train EER':^11} | {'Dev EER':^10} | {'Eval EER':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*50)  \n",
        "        \n",
        "        min_loss = 100\n",
        "        num_no_imp = 0\n",
        "        for i in range(config['num_epochs']):\n",
        "            epoch = i + 1\n",
        "            epoch_start_time = time.time()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "            \n",
        "            pending_model.train()\n",
        "            for signals, labels in train_loader:\n",
        "                signals = signals.to(self.active_device)\n",
        "                labels = labels.to(self.active_device)\n",
        "                \n",
        "                _, logits = pending_model(signals)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(logits, labels)\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "                \n",
        "            avg_loss = total_loss / num_batches\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Validation test.\n",
        "            dev_eer, _ = compute_eer(pending_model, dev_loader)\n",
        "            train_eer, _ = compute_eer(pending_model, train_loader)\n",
        "            eval_eer, _ = compute_eer(pending_model, eval_loader)\n",
        "            print(f\"{epoch:^7} | {avg_loss:^12.6f} | {train_eer:^9.2f} | {dev_eer:^9.2f} |  {eval_eer:^9.4f} | {epoch_time:^9.2f}\")\n",
        "                \n",
        "            if avg_loss < min_loss:\n",
        "                min_loss = avg_loss\n",
        "                num_no_imp = 0\n",
        "            else:\n",
        "                num_no_imp += 1\n",
        "                \n",
        "            if num_no_imp > config[\"early_stop_max_no_imp\"]:\n",
        "                print('early stop exit')\n",
        "                break\n",
        "            \n",
        "            if epoch < config[\"min_valid_epochs\"]:\n",
        "                continue\n",
        "            \n",
        "            if dev_eer < best_dev_eer:\n",
        "                best_dev_eer = dev_eer\n",
        "                best_dev_epoch = epoch\n",
        "                optimal_model = copy.deepcopy(pending_model)\n",
        "\n",
        "            if eval_eer < best_eval_eer:\n",
        "                best_eval_eer = eval_eer\n",
        "                best_eval_epoch = epoch\n",
        "        \n",
        "        print('AASIST trainer - end\\n')\n",
        "        print(\"Best Dev EER = {:.2f}\".format(best_dev_eer) + \", best epoch = \" + str(best_dev_epoch))\n",
        "        print(\"Best Eval Acc = {:.2f}\".format(best_eval_eer) + \", best epoch = \" + str(best_eval_epoch))\n",
        "        return pending_model, optimal_model, best_dev_epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rrJFG9hFO6p",
        "outputId": "1c78dad3-cc83-4491-c0f6-e9861f82dc46"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchcontrib in /usr/local/lib/python3.9/dist-packages (0.0.2)\n",
            "torchcontrib installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer()\n",
        "last_epoch_model, best_dev_eer_model, best_dev_eer_epoch = trainer.train(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMvmey-GBUl",
        "outputId": "a3191b6c-294b-4d48-da3f-cd559dfd693c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AASIST trainer - start\n",
            "Load samples\n",
            "Train Set size = 20288\n",
            "Dev Set size = 9922\n",
            "Eval Set size = 9796\n",
            "set optimizer & loss\n",
            "start training loops. #epochs = 15\n",
            " Epoch  |  Train Loss  |  Train EER  |  Dev EER   | Eval EER  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   0.693075   |   15.98   |   16.70   |   14.4073  |  765.73  \n",
            "   2    |   0.424156   |   6.45    |   8.48    |   10.1140  |  790.64  \n",
            "   3    |   0.267818   |   4.61    |   6.90    |   8.5320   |  789.16  \n",
            "   4    |   0.219517   |   3.17    |   4.96    |   5.8810   |  791.90  \n",
            "   5    |   0.173083   |   2.40    |   4.45    |   4.6832   |  792.12  \n",
            "   6    |   0.142940   |   2.15    |   5.34    |   4.5601   |  788.45  \n",
            "   7    |   0.123636   |   1.62    |   4.85    |   4.2387   |  791.34  \n",
            "   8    |   0.115699   |   1.62    |   3.33    |   3.4705   |  792.67  \n",
            "   9    |   0.104050   |   1.76    |   3.96    |   3.6198   |  792.21  \n",
            "  10    |   0.091661   |   1.79    |   2.75    |   3.7087   |  791.92  \n",
            "  11    |   0.084694   |   1.60    |   2.95    |   3.2186   |  792.46  \n",
            "  12    |   0.091008   |   1.62    |   2.57    |   3.2186   |  792.16  \n",
            "  13    |   0.081882   |   1.44    |   3.24    |   3.1411   |  792.27  \n",
            "  14    |   0.077437   |   1.68    |   3.35    |   2.9804   |  792.66  \n",
            "  15    |   0.075265   |   1.48    |   3.57    |   3.2323   |  792.84  \n",
            "AASIST trainer - end\n",
            "\n",
            "Best Dev EER = 2.57, best epoch = 12\n",
            "Best Eval Acc = 2.98, best epoch = 14\n"
          ]
        }
      ]
    }
  ]
}